1
00:00:00,630 --> 00:00:02,190
Hello and welcome.

2
00:00:02,190 --> 00:00:07,110
In this video, we'll look at k-Means
accuracy and characteristics.

3
00:00:07,110 --> 00:00:08,820
Let's get started.

4
00:00:08,820 --> 00:00:11,130
Let's define the algorithm
more concretely,

5
00:00:11,130 --> 00:00:13,020
before we talk about its accuracy.

6
00:00:14,180 --> 00:00:20,780
A k-Means algorithm works by randomly
placing k centroids, one for each cluster.

7
00:00:20,780 --> 00:00:24,070
The farther apart the clusters are placed,
the better.

8
00:00:24,070 --> 00:00:27,870
The next step is to calculate
the distance of each data point or

9
00:00:27,870 --> 00:00:30,320
object from the centroids.

10
00:00:30,320 --> 00:00:35,160
Euclidean distance is used to measure the
distance from the object to the centroid.

11
00:00:35,160 --> 00:00:38,140
Please note, however,
that you can also use

12
00:00:38,140 --> 00:00:42,470
different types of distance measurements,
not just Euclidean distance.

13
00:00:42,470 --> 00:00:46,240
Euclidean distance is used
because it's the most popular.

14
00:00:46,240 --> 00:00:53,060
Then, assign each data point or object to
its closest centroid creating a group.

15
00:00:53,060 --> 00:00:57,150
Next, once each data point has
been classified to a group,

16
00:00:57,150 --> 00:01:00,750
recalculate the position
of the k centroids.

17
00:01:00,750 --> 00:01:06,520
The new centroid position is determined
by the mean of all points in the group.

18
00:01:06,520 --> 00:01:11,420
Finally, this continues until
the centroids no longer move.

19
00:01:11,420 --> 00:01:13,010
Now, the questions is,

20
00:01:13,010 --> 00:01:18,080
how can we evaluate the goodness
of the clusters formed by k-Means?

21
00:01:18,080 --> 00:01:23,010
In other words, how do we calculate
the accuracy of k-Means clustering?

22
00:01:23,010 --> 00:01:27,206
One way is to compare the clusters with
the ground truth, if it's available.

23
00:01:27,206 --> 00:01:32,170
However, because k-Means is
an unsupervised algorithm

24
00:01:32,170 --> 00:01:36,730
we usually don't have ground truth
in real world problems to be used.

25
00:01:36,730 --> 00:01:41,140
But there is still a way to
say how bad each cluster is,

26
00:01:41,140 --> 00:01:43,920
based on the objective of the k-Means.

27
00:01:43,920 --> 00:01:49,280
This value is the average distance
between data points within a cluster.

28
00:01:49,280 --> 00:01:54,500
Also, average of the distances of data
points from their cluster centroids

29
00:01:54,500 --> 00:01:59,240
can be used as a metric of error for
the clustering algorithm.

30
00:01:59,240 --> 00:02:02,920
Essentially, determining the number
of clusters in a data set, or

31
00:02:02,920 --> 00:02:09,200
k as in the k-Means algorithm,
is a frequent problem in data clustering.

32
00:02:09,200 --> 00:02:14,140
The correct choice of K is often ambiguous
because it's very dependent on the shape

33
00:02:14,140 --> 00:02:16,790
and scale of the distribution
of points in a dataset.

34
00:02:16,790 --> 00:02:20,970
There are some approaches to
address this problem, but

35
00:02:20,970 --> 00:02:25,210
one of the techniques that is commonly
used is to run the clustering across

36
00:02:25,210 --> 00:02:29,670
the different values of K and looking
at a metric of accuracy for clustering.

37
00:02:30,700 --> 00:02:36,360
This metric can be mean, distance between
data points and their cluster's centroid.

38
00:02:36,360 --> 00:02:40,010
Which indicate how dense
our clusters are or,

39
00:02:40,010 --> 00:02:43,790
to what extent we minimize
the error of clustering.

40
00:02:43,790 --> 00:02:48,510
Then, looking at the change of this
metric, we can find the best value for K.

41
00:02:49,690 --> 00:02:53,660
But the problem is that with
increasing the number of clusters,

42
00:02:53,660 --> 00:02:58,020
the distance of centroids to
data points will always reduce.

43
00:02:58,020 --> 00:03:03,060
This means increasing K will
always decrease the error.

44
00:03:03,060 --> 00:03:07,860
So, the value of the metric as
a function of K is plotted and

45
00:03:07,860 --> 00:03:13,420
the elbow point is determined where
the rate of decrease sharply shifts.

46
00:03:13,420 --> 00:03:15,980
It is the right K for clustering.

47
00:03:15,980 --> 00:03:19,460
This method is called the elbow method.

48
00:03:19,460 --> 00:03:22,565
So let's recap k-Means clustering.

49
00:03:22,565 --> 00:03:27,060
k-Means is a partition-based
clustering which is A,

50
00:03:27,060 --> 00:03:30,538
relatively efficient on medium and
large sized data sets.

51
00:03:30,538 --> 00:03:33,160
B, produces sphere-like

52
00:03:33,160 --> 00:03:37,220
clusters because the clusters
are shaped around the centroids.

53
00:03:37,220 --> 00:03:42,710
And C, its drawback is that we should
pre-specify the number of clusters,

54
00:03:42,710 --> 00:03:45,330
and this is not an easy task.

55
00:03:45,330 --> 00:03:46,200
Thanks for watching.