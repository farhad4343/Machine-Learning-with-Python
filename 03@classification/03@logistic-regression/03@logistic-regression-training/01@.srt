1
00:00:00,000 --> 00:00:03,760
Hello and welcome. In this video,

2
00:00:03,760 --> 00:00:07,600
we will learn more about training a logistic regression model.

3
00:00:07,600 --> 00:00:10,410
Also, we will be discussing how to change

4
00:00:10,410 --> 00:00:14,115
the parameters of the model to better estimate the outcome.

5
00:00:14,115 --> 00:00:18,580
Finally, we talk about the cost function and gradient descent in

6
00:00:18,580 --> 00:00:23,480
logistic regression as a way to optimize the model. So, let's start.

7
00:00:23,480 --> 00:00:25,750
The main objective of training and

8
00:00:25,750 --> 00:00:29,590
logistic regression is to change the parameters of the model,

9
00:00:29,590 --> 00:00:34,120
so as to be the best estimation of the labels of the samples in the dataset.

10
00:00:34,120 --> 00:00:38,455
For example, the customer churn. How do we do that?

11
00:00:38,455 --> 00:00:42,065
In brief, first we have to look at the cost function,

12
00:00:42,065 --> 00:00:47,045
and see what the relation is between the cost function and the parameters theta.

13
00:00:47,045 --> 00:00:50,615
So, we should formulate the cost function,

14
00:00:50,615 --> 00:00:54,770
then using the derivative of the cost function we can find how

15
00:00:54,770 --> 00:00:59,090
to change the parameters to reduce the cost or rather the error.

16
00:00:59,090 --> 00:01:01,745
Let's dive into it to see how it works.

17
00:01:01,745 --> 00:01:05,330
But before I explain it I should highlight for you that it needs

18
00:01:05,330 --> 00:01:09,020
some basic mathematical background to understand it.

19
00:01:09,020 --> 00:01:13,690
However, you shouldn't worry about it as most data science languages like Python,

20
00:01:13,690 --> 00:01:19,725
R and Scala have some packages or libraries that calculate these parameters for you.

21
00:01:19,725 --> 00:01:22,475
So, let's take a look at it.

22
00:01:22,475 --> 00:01:26,875
Let's first find the cost function equation for a sample case.

23
00:01:26,875 --> 00:01:31,055
To do this we can use one of the customers in the churn problem.

24
00:01:31,055 --> 00:01:34,985
There's normally a general equation for calculating the cost.

25
00:01:34,985 --> 00:01:37,740
The cost function is the difference between

26
00:01:37,740 --> 00:01:42,380
the actual values of y and our model output y hat.

27
00:01:42,380 --> 00:01:46,705
This is a general rule for most cost functions in machine learning.

28
00:01:46,705 --> 00:01:51,990
We can show this as the cost of our model comparing it with actual labels,

29
00:01:51,990 --> 00:01:54,390
which is the difference between the predicted value of

30
00:01:54,390 --> 00:01:57,440
our model and actual value of the target field,

31
00:01:57,440 --> 00:02:02,535
where the predicted value of our model is sigmoid of theta transpose x.

32
00:02:02,535 --> 00:02:05,880
Usually the square of this equation is used because of

33
00:02:05,880 --> 00:02:09,995
the possibility of the negative result and for the sake of simplicity,

34
00:02:09,995 --> 00:02:15,569
half of this value is considered as the cost function through the derivative process.

35
00:02:15,569 --> 00:02:20,005
Now, we can write the cost function for all the samples in our training set.

36
00:02:20,005 --> 00:02:23,400
For example, for all customers we can write

37
00:02:23,400 --> 00:02:27,090
it as the average sum of the cost functions of all cases.

38
00:02:27,090 --> 00:02:31,240
It is also called the mean squared error and as it is

39
00:02:31,240 --> 00:02:36,025
a function of a parameter vector theta it is shown as J of theta.

40
00:02:36,025 --> 00:02:39,505
Okay good, we have the cost function.

41
00:02:39,505 --> 00:02:42,130
Now, how do we find or set

42
00:02:42,130 --> 00:02:45,705
the best weights or parameters that minimize this cost function?

43
00:02:45,705 --> 00:02:49,330
The answer is, we should calculate the minimum point of

44
00:02:49,330 --> 00:02:53,990
this cost function and it will show us the best parameters for our model.

45
00:02:53,990 --> 00:02:58,815
Although we can find the minimum point of a function using the derivative of a function,

46
00:02:58,815 --> 00:03:03,990
there's not an easy way to find the global minimum point for such an equation.

47
00:03:03,990 --> 00:03:07,060
Given this complexity, describing how to reach

48
00:03:07,060 --> 00:03:11,095
the global minimum for this equation is outside the scope of this video.

49
00:03:11,095 --> 00:03:13,285
So, what is the solution?

50
00:03:13,285 --> 00:03:16,870
Well we should find another cost function instead,

51
00:03:16,870 --> 00:03:21,785
one which has the same behavior but it's easier to find its minimum point.

52
00:03:21,785 --> 00:03:25,600
Let's plot the desirable cost function for our model.

53
00:03:25,600 --> 00:03:28,435
Recall that our model is y hat.

54
00:03:28,435 --> 00:03:32,690
Our actual value is y which equals zero or one,

55
00:03:32,690 --> 00:03:35,680
and our model tries to estimate it as we want

56
00:03:35,680 --> 00:03:38,915
to find a simple cost function for our model.

57
00:03:38,915 --> 00:03:43,625
For a moment assume that our desired value for y is one.

58
00:03:43,625 --> 00:03:48,430
This means our model is best if it estimates y equals one.

59
00:03:48,430 --> 00:03:51,280
In this case, we need a cost function that

60
00:03:51,280 --> 00:03:54,730
returns zero if the outcome of our model is one,

61
00:03:54,730 --> 00:03:57,385
which is the same as the actual label.

62
00:03:57,385 --> 00:04:03,150
And the cost should keep increasing as the outcome of our model gets farther from one.

63
00:04:03,150 --> 00:04:09,210
And cost should be very large if the outcome of our model is close to zero.

64
00:04:09,210 --> 00:04:15,180
We can see that the minus log function provides such a cost function for us.

65
00:04:15,180 --> 00:04:21,310
It means if the actual value is one and the model also predicts one,

66
00:04:21,310 --> 00:04:24,620
the minus log function returns zero cost.

67
00:04:24,620 --> 00:04:28,330
But if the prediction is smaller than one,

68
00:04:28,330 --> 00:04:32,245
the minus log function returns a larger cost value.

69
00:04:32,245 --> 00:04:35,710
So, we can use the minus log function for

70
00:04:35,710 --> 00:04:39,785
calculating the cost of our logistic regression model.

71
00:04:39,785 --> 00:04:41,380
So, if you recall,

72
00:04:41,380 --> 00:04:43,780
we previously noted that in general it is

73
00:04:43,780 --> 00:04:47,230
difficult to calculate the derivative of the cost function.

74
00:04:47,230 --> 00:04:51,840
Well, we can now change it with the minus log of our model.

75
00:04:51,840 --> 00:04:57,165
We can easily prove that in the case that desirable y is one,

76
00:04:57,165 --> 00:05:01,350
the cost can be calculated as minus log y hat,

77
00:05:01,350 --> 00:05:05,710
and in the case that desirable y is zero the cost

78
00:05:05,710 --> 00:05:10,580
can be calculated as minus log one minus y hat.

79
00:05:10,580 --> 00:05:16,255
Now, we can plug it into our total cost function and rewrite it as this function.

80
00:05:16,255 --> 00:05:20,485
So, this is the logistic regression cost function.

81
00:05:20,485 --> 00:05:24,340
As you can see for yourself it penalizes situations in which

82
00:05:24,340 --> 00:05:28,485
the class is zero and the model output is one, and vice versa.

83
00:05:28,485 --> 00:05:32,860
Remember however that y hat does not return a class as

84
00:05:32,860 --> 00:05:39,035
output but it's a value of zero or one which should be assumed as a probability.

85
00:05:39,035 --> 00:05:42,580
Now, we can easily use this function to find the parameters of

86
00:05:42,580 --> 00:05:46,525
our model in such a way as to minimize the cost.

87
00:05:46,525 --> 00:05:49,520
Okay, let's recap what we have done.

88
00:05:49,520 --> 00:05:54,400
Our objective was to find a model that best estimates the actual labels.

89
00:05:54,400 --> 00:06:00,015
Finding the best model means finding the best parameters theta for that model.

90
00:06:00,015 --> 00:06:02,275
So, the first question was,

91
00:06:02,275 --> 00:06:05,885
how do we find the best parameters for our model.

92
00:06:05,885 --> 00:06:10,755
Well, by finding and minimizing the cost function of our model.

93
00:06:10,755 --> 00:06:15,335
In other words, to minimize the J of theta we just defined.

94
00:06:15,335 --> 00:06:17,375
The next question is,

95
00:06:17,375 --> 00:06:19,925
how do we minimize the cost function?

96
00:06:19,925 --> 00:06:23,955
The answer is, using an optimization approach.

97
00:06:23,955 --> 00:06:26,980
There are different optimization approaches,

98
00:06:26,980 --> 00:06:32,550
but we use one of the most famous and effective approaches here, gradient descent.

99
00:06:32,550 --> 00:06:34,570
The next question is,

100
00:06:34,570 --> 00:06:36,865
what is gradient descent?

101
00:06:36,865 --> 00:06:39,610
Generally, gradient descent is

102
00:06:39,610 --> 00:06:43,030
an iterative approach to finding the minimum of a function.

103
00:06:43,030 --> 00:06:48,520
Specifically in our case gradient descent is a technique to use the derivative of

104
00:06:48,520 --> 00:06:54,100
a cost function to change the parameter values to minimize the cost or error.

105
00:06:54,100 --> 00:06:56,285
Let's see how it works.

106
00:06:56,285 --> 00:06:59,890
The main objective of gradient descent is to change

107
00:06:59,890 --> 00:07:03,375
the parameter values so as to minimize the cost.

108
00:07:03,375 --> 00:07:05,825
How can gradient descent do that?

109
00:07:05,825 --> 00:07:11,175
Think of the parameters or weights in our model to be in a two-dimensional space.

110
00:07:11,175 --> 00:07:13,530
For example, theta one,

111
00:07:13,530 --> 00:07:17,535
theta two for two feature sets, age and income.

112
00:07:17,535 --> 00:07:19,400
Recall the cost function,

113
00:07:19,400 --> 00:07:22,540
J, that we discussed in the previous slides.

114
00:07:22,540 --> 00:07:25,560
We need to minimize the cost function J

115
00:07:25,560 --> 00:07:29,260
which is a function of variables theta one and theta two.

116
00:07:29,260 --> 00:07:32,730
So, let's add a dimension for the observed cost,

117
00:07:32,730 --> 00:07:34,940
or error, J function.

118
00:07:34,940 --> 00:07:40,570
Let's assume that if we plot the cost function based on all possible values of theta one,

119
00:07:40,570 --> 00:07:43,975
theta two, we can see something like this.

120
00:07:43,975 --> 00:07:48,390
It represents the error value for different values of parameters,

121
00:07:48,390 --> 00:07:51,705
that is error which is a function of the parameters.

122
00:07:51,705 --> 00:07:56,690
This is called your error curve or error bowl of your cost function.

123
00:07:56,690 --> 00:07:59,960
Recall that we want to use this error bowl to find

124
00:07:59,960 --> 00:08:04,335
the best parameter values that result in minimizing the cost value.

125
00:08:04,335 --> 00:08:06,375
Now, the question is,

126
00:08:06,375 --> 00:08:09,865
which point is the best point for your cost function.

127
00:08:09,865 --> 00:08:14,850
Yes, you should try to minimize your position on the error curve.

128
00:08:14,850 --> 00:08:16,985
So, what should you do?

129
00:08:16,985 --> 00:08:21,380
You have to find the minimum value of the cost by changing the parameters.

130
00:08:21,380 --> 00:08:23,065
But which way?

131
00:08:23,065 --> 00:08:27,045
Will you add some value to your weights or deduct some value?

132
00:08:27,045 --> 00:08:29,635
And how much would that value be?

133
00:08:29,635 --> 00:08:34,930
You can select random parameter values that locate a point on the bowl.

134
00:08:34,930 --> 00:08:38,550
You can think of our starting point being the yellow point.

135
00:08:38,550 --> 00:08:43,220
You change the parameters by delta theta one and delta theta two,

136
00:08:43,220 --> 00:08:45,810
and take one step on the surface.

137
00:08:45,810 --> 00:08:49,340
Let's assume we go down one step in the bowl.

138
00:08:49,340 --> 00:08:53,705
As long as we are going downwards we can go one more step.

139
00:08:53,705 --> 00:08:56,765
The steeper the slope the further we can step,

140
00:08:56,765 --> 00:08:59,375
and we can keep taking steps.

141
00:08:59,375 --> 00:09:02,970
As we approach the lowest point the slope diminishes,

142
00:09:02,970 --> 00:09:07,600
so we can take smaller steps until we reach a flat surface.

143
00:09:07,600 --> 00:09:13,205
This is the minimum point of our curve and the optimum theta one, theta two.

144
00:09:13,205 --> 00:09:15,340
What are these steps really?

145
00:09:15,340 --> 00:09:19,460
I mean in which direction should we take these steps to make sure we descend,

146
00:09:19,460 --> 00:09:22,055
and how big should the steps be?

147
00:09:22,055 --> 00:09:24,860
To find the direction and size of these steps,

148
00:09:24,860 --> 00:09:28,330
in other words to find how to update the parameters,

149
00:09:28,330 --> 00:09:32,740
you should calculate the gradient of the cost function at that point.

150
00:09:32,740 --> 00:09:36,695
The gradient is the slope of the surface at every point

151
00:09:36,695 --> 00:09:41,455
and the direction of the gradient is the direction of the greatest uphill.

152
00:09:41,455 --> 00:09:43,190
Now, the question is,

153
00:09:43,190 --> 00:09:47,980
how do we calculate the gradient of a cost function at a point?

154
00:09:47,980 --> 00:09:50,710
If you select a random point on this surface,

155
00:09:50,710 --> 00:09:52,435
for example the yellow point,

156
00:09:52,435 --> 00:09:54,890
and take the partial derivative of J of

157
00:09:54,890 --> 00:09:58,700
theta with respect to each parameter at that point,

158
00:09:58,700 --> 00:10:02,820
it gives you the slope of the move for each parameter at that point.

159
00:10:02,820 --> 00:10:06,859
Now, if we move in the opposite direction of that slope,

160
00:10:06,859 --> 00:10:10,305
it guarantees that we go down in the error curve.

161
00:10:10,305 --> 00:10:15,420
For example, if we calculate the derivative of J with respect to theta one,

162
00:10:15,420 --> 00:10:18,635
we find out that it is a positive number.

163
00:10:18,635 --> 00:10:22,945
This indicates that function is increasing as theta one increases.

164
00:10:22,945 --> 00:10:26,120
So, to decrease J we should move in the opposite

165
00:10:26,120 --> 00:10:28,930
direction.This means to move in

166
00:10:28,930 --> 00:10:32,940
the direction of the negative derivative for theta one, i.e.

167
00:10:32,940 --> 00:10:38,355
slope. We have to calculate it for other parameters as well at each step.

168
00:10:38,355 --> 00:10:42,570
The gradient value also indicates how big of a step to take.

169
00:10:42,570 --> 00:10:47,545
If the slope is large we should take a large step because we are far from the minimum.

170
00:10:47,545 --> 00:10:51,120
If the slope is small we should take a smaller step.

171
00:10:51,120 --> 00:10:54,880
Gradient descent takes increasingly smaller steps

172
00:10:54,880 --> 00:10:57,560
towards the minimum with each iteration.

173
00:10:57,560 --> 00:11:03,430
The partial derivative of the cost function J is calculated using this expression.

174
00:11:03,430 --> 00:11:07,665
If you want to know how the derivative of the J function is calculated,

175
00:11:07,665 --> 00:11:12,060
you need to know the derivative concept which is beyond our scope here.

176
00:11:12,060 --> 00:11:16,090
But to be honest you don't really need to remember all the details about

177
00:11:16,090 --> 00:11:20,189
it as you can easily use this equation to calculate the gradients.

178
00:11:20,189 --> 00:11:24,430
So, in a nutshell this equation returns the slope of

179
00:11:24,430 --> 00:11:29,400
that point and we should update the parameter in the opposite direction of the slope.

180
00:11:29,400 --> 00:11:33,310
A vector of all these slopes is the gradient vector

181
00:11:33,310 --> 00:11:37,615
and we can use this vector to change or update all the parameters.

182
00:11:37,615 --> 00:11:42,530
We take the previous values of the parameters and subtract the error derivative.

183
00:11:42,530 --> 00:11:48,010
This results in the new parameters for theta that we know will decrease the cost.

184
00:11:48,010 --> 00:11:53,010
Also we multiply the gradient value by a constant value mu,

185
00:11:53,010 --> 00:11:55,090
which is called the learning rate.

186
00:11:55,090 --> 00:12:00,280
Learning rate, gives us additional control on how fast we move on the surface.

187
00:12:00,280 --> 00:12:02,955
In sum, we can simply say,

188
00:12:02,955 --> 00:12:07,600
gradient descent is like taking steps in the current direction of the slope,

189
00:12:07,600 --> 00:12:11,235
and the learning rate is like the length of the step you take.

190
00:12:11,235 --> 00:12:14,255
So, these would be our new parameters.

191
00:12:14,255 --> 00:12:19,460
Notice that it's an iterative operation and in each iteration we update

192
00:12:19,460 --> 00:12:22,100
the parameters and minimize the cost until

193
00:12:22,100 --> 00:12:26,165
the algorithm converge is on an acceptable minimum.

194
00:12:26,165 --> 00:12:29,705
Okay, let's recap what we have done to this point by

195
00:12:29,705 --> 00:12:33,320
going through the training algorithm again, step-by-step.

196
00:12:33,320 --> 00:12:38,420
Step one, we initialize the parameters with random values.

197
00:12:38,420 --> 00:12:43,890
Step two, we feed the cost function with the training set and calculate the cost.

198
00:12:43,890 --> 00:12:49,115
We expect a high error rate as the parameters are set randomly.

199
00:12:49,115 --> 00:12:52,440
Step three, we calculate the gradient of

200
00:12:52,440 --> 00:12:56,800
the cost function keeping in mind that we have to use a partial derivative.

201
00:12:56,800 --> 00:13:00,120
So, to calculate the gradient vector we need

202
00:13:00,120 --> 00:13:04,055
all the training data to feed the equation for each parameter.

203
00:13:04,055 --> 00:13:07,655
Of course, this is an expensive part of the algorithm,

204
00:13:07,655 --> 00:13:09,965
but there are some solutions for this.

205
00:13:09,965 --> 00:13:14,975
Step four, we update the weights with new parameter values.

206
00:13:14,975 --> 00:13:20,580
Step five, here we go back to step two and feed the cost function again,

207
00:13:20,580 --> 00:13:22,555
which has new parameters.

208
00:13:22,555 --> 00:13:24,760
As was explained earlier,

209
00:13:24,760 --> 00:13:28,960
we expect less error as we are going down the error surface.

210
00:13:28,960 --> 00:13:32,040
We continue this loop until we reach a short value

211
00:13:32,040 --> 00:13:35,510
of cost or some limited number of iterations.

212
00:13:35,510 --> 00:13:40,900
Step six, the parameter should be roughly found after some iterations.

213
00:13:40,900 --> 00:13:43,860
This means the model is ready and we can use it to

214
00:13:43,860 --> 00:13:47,640
predict the probability of a customer staying or leaving.

215
00:13:47,640 --> 00:13:50,730
Thanks for watching this video.